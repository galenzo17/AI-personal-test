{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnlz/ws9BlY0OCH/GkYCpW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galenzo17/AI-personal-test/blob/main/fine_tune_on_fly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "l-9CVYLZ6SSS",
        "outputId": "9faf00f3-2636-45ad-b2ed-c7200ab2f3fc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 133) (<ipython-input-1-70dc2ab7a449>, line 133)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-70dc2ab7a449>\"\u001b[0;36m, line \u001b[0;32m133\u001b[0m\n\u001b[0;31m    logging.info(f'Finalizado el ajuste fino para la división {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_file\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 133)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import glob\n",
        "import shutil\n",
        "import logging\n",
        "import subprocess\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configuración\n",
        "class cfg:\n",
        "    # Modelo\n",
        "    model_path = '/kaggle/input/qwen2.5/transformers/0.5b-instruct/1'\n",
        "    input_lora_path = '/kaggle/input/loras/transformers/qwen2.5-0.5b-instruct/8'\n",
        "    prompt_version = 'output-from-examples-v1'\n",
        "    merged_model_path = '/kaggle/tmp/qwen_merged_model'\n",
        "    grid_encoder = 'GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))'\n",
        "    max_model_len = 10240\n",
        "    # Conjunto de datos\n",
        "    dataset_path = '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json'\n",
        "    n_splits = 100\n",
        "    split_size = 100 // n_splits\n",
        "    # Parámetros de ajuste fino\n",
        "    total_train_steps = 32000\n",
        "    max_steps = total_train_steps // n_splits\n",
        "    learning_rate = 8e-5\n",
        "    lr_scheduler_type = \"linear\"\n",
        "    batch_size = 1\n",
        "    max_seq_len = 5120\n",
        "    # Parámetros de inferencia\n",
        "    predictions_per_task = 96\n",
        "    inference_timeout = \"12m\"\n",
        "    # Ensamble\n",
        "    ensemble_with_2020 = True\n",
        "\n",
        "# Verificación de ejecución de prueba\n",
        "is_dry_run = (cfg.dataset_path == '/kaggle/input/arc-prize-2024/arc-agi_test_challenges.json' and\n",
        "              not os.getenv('KAGGLE_IS_COMPETITION_RERUN'))\n",
        "if is_dry_run:\n",
        "    print('Esta es una ejecución de prueba; no se realizará inferencia ni instalación de paquetes.')\n",
        "\n",
        "# Validación de la versión del prompt\n",
        "if int(cfg.input_lora_path.split('/')[-1]) < 18 and cfg.input_lora_path.startswith('/kaggle/input/loras/transformers/qwen2-0.5b'):\n",
        "    assert cfg.prompt_version == 'output-from-examples-v0'\n",
        "else:\n",
        "    assert cfg.prompt_version == 'output-from-examples-v1'\n",
        "\n",
        "# Configuración de registros\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
        "\n",
        "# Lanzamiento en segundo plano de la solución 2020\n",
        "if not is_dry_run and cfg.ensemble_with_2020:\n",
        "    print('Lanzando la solución 2020 en segundo plano')\n",
        "    args = [\n",
        "        'python',\n",
        "        '/kaggle/input/arc24-source-code/full_2020_solution.py',\n",
        "        f'--dataset_filepath={cfg.dataset_path}',\n",
        "        '--icecuber_output_filepath=icecuber_submission.json',\n",
        "        '--dsl_output_filepath=submission_program_search.json'\n",
        "    ]\n",
        "    full_2020_solution_process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Instalación de bibliotecas y monitor de recursos\n",
        "if not is_dry_run:\n",
        "    subprocess.run(['bash', '/kaggle/input/arc24-source-code/install_libraries.sh'], check=True)\n",
        "    from arc24.utils import ResourceMonitor\n",
        "    monitor = ResourceMonitor(interval=1)\n",
        "    monitor.start()\n",
        "\n",
        "# Preparación de datos para entrenamiento\n",
        "if not is_dry_run:\n",
        "    single_task_datasets_path = 'single_task_datasets'\n",
        "    os.makedirs(single_task_datasets_path, exist_ok=True)\n",
        "    with open(cfg.dataset_path, 'r') as f:\n",
        "        items = list(json.load(f).items())\n",
        "    assert len(items) % cfg.split_size == 0\n",
        "    for batch_idx in tqdm(range(len(items) // cfg.split_size), desc='Creando conjuntos de datos de una sola tarea'):\n",
        "        data = dict(items[batch_idx * cfg.split_size: (batch_idx + 1) * cfg.split_size])\n",
        "        assert len(data) == cfg.split_size\n",
        "        task_id = list(data.keys())[0]\n",
        "        with open(os.path.join(single_task_datasets_path, f'{task_id}.json'), 'w') as f:\n",
        "            json.dump(data, f)\n",
        "    print(f'Contenido de {single_task_datasets_path}:')\n",
        "    print(os.listdir(single_task_datasets_path))\n",
        "\n",
        "    training_datasets_path = 'single_task_training_datasets'\n",
        "    os.makedirs(training_datasets_path, exist_ok=True)\n",
        "    dataset_filepaths = glob.glob(os.path.join(single_task_datasets_path, '*.json'))\n",
        "    for dataset_filepath in tqdm(dataset_filepaths, desc='Creando conjuntos de datos de entrenamiento ttft'):\n",
        "        subprocess.run([\n",
        "            'python', '/kaggle/input/arc24-source-code/create_n-1_dataset.py',\n",
        "            dataset_filepath,\n",
        "            os.path.join(training_datasets_path, os.path.basename(dataset_filepath))\n",
        "        ], check=True)\n",
        "\n",
        "# Función para limpiar salidas de entrenamiento excepto el adaptador\n",
        "def clean_train_output_except_adapter(output_dir):\n",
        "    patterns = [\n",
        "        '*/*.pth', '*/*.pt', '*/*.md', '*/*.txt', '*/*.bin', '*/token*',\n",
        "        '*/added_tokens.json', '*/special_tokens_map.json', '*/vocab.json', '*/trainer_state.json'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        for file in glob.glob(os.path.join(output_dir, pattern)):\n",
        "            os.remove(file)\n",
        "\n",
        "# Ajuste fino en tiempo de prueba\n",
        "if not is_dry_run:\n",
        "    dataset_filepaths = sorted(glob.glob(os.path.join(training_datasets_path, '*.json')))\n",
        "    checkpoints_folder = '/kaggle/tmp/checkpoints'\n",
        "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
        "    for dataset_filepath in tqdm(dataset_filepaths, desc='Ajustando modelos'):\n",
        "        output_dir = os.path.join(checkpoints_folder, os.path.splitext(os.path.basename(dataset_filepath))[0])\n",
        "        subprocess.run([\n",
        "            'python', '/kaggle/input/arc24-source-code/fine-tuning.py',\n",
        "            f'--model_path={cfg.model_path}',\n",
        "            f'--adapter_path={cfg.input_lora_path}',\n",
        "            f'--output_dir={output_dir}',\n",
        "            '--train_datasets', dataset_filepath, cfg.prompt_version,\n",
        "            '--val_dataset', dataset_filepath, cfg.prompt_version,\n",
        "            f'--max_steps={cfg.max_steps}',\n",
        "            f'--eval_steps={cfg.max_steps * 2}',\n",
        "            f'--max_seq_len={cfg.max_seq_len}',\n",
        "            f'--learning_rate={cfg.learning_rate}',\n",
        "            f'--lr_scheduler_type={cfg.lr_scheduler_type}',\n",
        "            f'--batch_size={cfg.batch_size}',\n",
        "            '--report_to=tensorboard',\n",
        "            f'--grid_encoder={cfg.grid_encoder}',\n",
        "            '--remove_train_samples_to_fit_max_seq_len',\n",
        "            '--torch_dtype=float16',\n",
        "            '--no-verbose'\n",
        "        ], check=True)\n",
        "        clean_train_output_except_adapter(output_dir)\n",
        "        logging.info(f'Finalizado el ajuste fino para la división {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_file\n",
        "::contentReference[oaicite:0]{index=0}\n",
        "\n"
      ]
    }
  ]
}