{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVKmSdRIQfSvlrpWFJLq0u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galenzo17/AI-personal-test/blob/main/agus_bench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VEhTnSW4hci"
      },
      "outputs": [],
      "source": [
        "# Instalar las bibliotecas necesarias\n",
        "!pip install openai huggingface_hub requests\n",
        "\n",
        "import openai\n",
        "import subprocess\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "from difflib import SequenceMatcher\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# -------------------- Configuración de Autenticación -------------------- #\n",
        "\n",
        "# Cargar claves API desde variables de entorno para mayor seguridad\n",
        "# Puedes establecer estas variables en Colab usando:\n",
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = 'tu_clave_api'\n",
        "# os.environ['HUGGINGFACE_API_KEY'] = 'tu_clave_api_hf'\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')  # Clave API de OpenAI\n",
        "HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')  # Clave API de Hugging Face\n",
        "\n",
        "# Configurar la clave API de OpenAI\n",
        "if OPENAI_API_KEY:\n",
        "    openai.api_key = OPENAI_API_KEY\n",
        "else:\n",
        "    print(\"⚠️ OpenAI API key not found. OpenAI models will be skipped.\")\n",
        "\n",
        "# -------------------- Definición de Clases para Modelos -------------------- #\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkQuestion:\n",
        "    question: str\n",
        "    expected_answer: str\n",
        "\n",
        "@dataclass\n",
        "class ModelResult:\n",
        "    question: str\n",
        "    expected_answer: str\n",
        "    correct_responses: int\n",
        "    consistency: float\n",
        "    answers: List[Dict[str, Any]] = field(default_factory=list)\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "\n",
        "    def query(self, prompt: str) -> Optional[str]:\n",
        "        raise NotImplementedError(\"Esta método debe ser implementado por subclases.\")\n",
        "\n",
        "class OpenAIModel(BaseModel):\n",
        "    def __init__(self, name: str, model_name: str, temperature: float = 0):\n",
        "        super().__init__(name)\n",
        "        self.model_name = model_name\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def query(self, prompt: str) -> Optional[str]:\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=self.model_name,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=self.temperature,\n",
        "                max_tokens=150\n",
        "            )\n",
        "            answer = response.choices[0].message['content'].strip()\n",
        "            return answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error al consultar OpenAI model '{self.name}': {e}\")\n",
        "            return None\n",
        "\n",
        "class HuggingFaceModel(BaseModel):\n",
        "    def __init__(self, name: str, model_id: str, huggingface_api_key: Optional[str] = None):\n",
        "        super().__init__(name)\n",
        "        self.model_id = model_id\n",
        "        self.hf_api_key = huggingface_api_key\n",
        "\n",
        "    def query(self, prompt: str) -> Optional[str]:\n",
        "        headers = {}\n",
        "        if self.hf_api_key:\n",
        "            headers[\"Authorization\"] = f\"Bearer {self.hf_api_key}\"\n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"options\": {\"use_cache\": False}\n",
        "        }\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                f\"https://api-inference.huggingface.co/models/{self.model_id}\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                # Manejar diferentes formatos de respuesta\n",
        "                if isinstance(data, list) and 'generated_text' in data[0]:\n",
        "                    return data[0]['generated_text'].strip()\n",
        "                elif isinstance(data, dict) and 'generated_text' in data:\n",
        "                    return data['generated_text'].strip()\n",
        "                else:\n",
        "                    return json.dumps(data)\n",
        "            else:\n",
        "                print(f\"Error en Hugging Face model '{self.name}': {response.status_code} - {response.text}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Exception al consultar Hugging Face model '{self.name}': {e}\")\n",
        "            return None\n",
        "\n",
        "class OllamaModel(BaseModel):\n",
        "    def __init__(self, name: str, model_command: str):\n",
        "        super().__init__(name)\n",
        "        self.model_command = model_command  # Comando completo para ejecutar el modelo\n",
        "\n",
        "    def query(self, prompt: str) -> Optional[str]:\n",
        "        try:\n",
        "            # Ejecutar el comando de Ollama con el prompt como entrada\n",
        "            process = subprocess.Popen(\n",
        "                self.model_command,\n",
        "                shell=True,\n",
        "                stdin=subprocess.PIPE,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            stdout, stderr = process.communicate(input=prompt, timeout=60)\n",
        "            if process.returncode == 0:\n",
        "                return stdout.strip()\n",
        "            else:\n",
        "                print(f\"Error en Ollama model '{self.name}': {stderr}\")\n",
        "                return None\n",
        "        except subprocess.TimeoutExpired:\n",
        "            process.kill()\n",
        "            print(f\"Timeout al consultar Ollama model '{self.name}'.\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Exception al consultar Ollama model '{self.name}': {e}\")\n",
        "            return None\n",
        "\n",
        "# -------------------- Definición de Funciones de Evaluación -------------------- #\n",
        "\n",
        "def evaluate_answer(model_answer: str, expected_answer: str, similarity_threshold: float = 0.8) -> bool:\n",
        "    \"\"\"\n",
        "    Evalúa la respuesta del modelo comparándola con la respuesta esperada.\n",
        "    Utiliza una métrica de similitud basada en SequenceMatcher.\n",
        "    \"\"\"\n",
        "    if not model_answer or not expected_answer:\n",
        "        return False\n",
        "\n",
        "    # Normalizar respuestas\n",
        "    model_answer_norm = model_answer.lower().strip()\n",
        "    expected_answer_norm = expected_answer.lower().strip()\n",
        "\n",
        "    # Comparación directa\n",
        "    if model_answer_norm == expected_answer_norm:\n",
        "        return True\n",
        "\n",
        "    # Comparación basada en similitud\n",
        "    similarity = SequenceMatcher(None, model_answer_norm, expected_answer_norm).ratio()\n",
        "    return similarity >= similarity_threshold\n",
        "\n",
        "# -------------------- Definición de Preguntas para el Benchmark -------------------- #\n",
        "\n",
        "benchmark_questions = [\n",
        "    BenchmarkQuestion(\n",
        "        question=\"¿Cuál es la capital de Francia?\",\n",
        "        expected_answer=\"París\"\n",
        "    ),\n",
        "    BenchmarkQuestion(\n",
        "        question=\"Resuelve la siguiente operación matemática: 15 * 12.\",\n",
        "        expected_answer=\"180\"\n",
        "    ),\n",
        "    BenchmarkQuestion(\n",
        "        question=\"¿Quién escribió 'Cien Años de Soledad'?\",\n",
        "        expected_answer=\"Gabriel García Márquez\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# -------------------- Definición de Modelos a Evaluar -------------------- #\n",
        "\n",
        "# Lista de modelos a evaluar. Puedes agregar o quitar modelos según tus necesidades.\n",
        "models_to_evaluate: List[BaseModel] = []\n",
        "\n",
        "# Ejemplo: Agregar un modelo de OpenAI\n",
        "if OPENAI_API_KEY:\n",
        "    models_to_evaluate.append(\n",
        "        OpenAIModel(\n",
        "            name=\"OpenAI-GPT-3.5-Turbo\",\n",
        "            model_name=\"gpt-3.5-turbo\",\n",
        "            temperature=0\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Ejemplo: Agregar un modelo de Hugging Face\n",
        "if HUGGINGFACE_API_KEY:\n",
        "    models_to_evaluate.append(\n",
        "        HuggingFaceModel(\n",
        "            name=\"HuggingFace-GPT-2\",\n",
        "            model_id=\"gpt2\",\n",
        "            huggingface_api_key=HUGGINGFACE_API_KEY\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Ejemplo: Agregar un modelo de Ollama (Debes ejecutar este script localmente)\n",
        "# Descomenta y configura las siguientes líneas si estás ejecutando el script localmente\n",
        "\"\"\"\n",
        "models_to_evaluate.append(\n",
        "    OllamaModel(\n",
        "        name=\"Ollama-Llama-3.2-1B-Instruct\",\n",
        "        model_command=\"ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\"\n",
        "    )\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# -------------------- Función para Ejecutar el Benchmark -------------------- #\n",
        "\n",
        "def run_benchmark(models: List[BaseModel], questions: List[BenchmarkQuestion], num_runs: int = 5) -> Dict[str, List[ModelResult]]:\n",
        "    results = {}\n",
        "    for model in models:\n",
        "        print(f\"\\n===== Evaluando el modelo: {model.name} =====\")\n",
        "        model_results = []\n",
        "        for q in questions:\n",
        "            print(f\"\\nPregunta: {q.question}\")\n",
        "            print(f\"Respuesta Esperada: {q.expected_answer}\")\n",
        "            correct_count = 0\n",
        "            answers = []\n",
        "            for run in range(1, num_runs + 1):\n",
        "                print(f\"  Consulta {run}/{num_runs}...\")\n",
        "                answer = model.query(q.question)\n",
        "                if answer is None:\n",
        "                    print(\"    No se pudo obtener una respuesta.\")\n",
        "                    answers.append({\n",
        "                        \"run\": run,\n",
        "                        \"answer\": None,\n",
        "                        \"is_correct\": False\n",
        "                    })\n",
        "                    continue\n",
        "                is_correct = evaluate_answer(answer, q.expected_answer)\n",
        "                if is_correct:\n",
        "                    correct_count += 1\n",
        "                answers.append({\n",
        "                    \"run\": run,\n",
        "                    \"answer\": answer,\n",
        "                    \"is_correct\": is_correct\n",
        "                })\n",
        "                status = \"✅ Correcta\" if is_correct else \"❌ Incorrecta\"\n",
        "                print(f\"    Respuesta: {answer} | {status}\")\n",
        "                time.sleep(1)  # Espera para evitar exceder los límites de la API\n",
        "            consistency = correct_count / num_runs\n",
        "            model_results.append(ModelResult(\n",
        "                question=q.question,\n",
        "                expected_answer=q.expected_answer,\n",
        "                correct_responses=correct_count,\n",
        "                consistency=consistency,\n",
        "                answers=answers\n",
        "            ))\n",
        "        results[model.name] = model_results\n",
        "    return results\n",
        "\n",
        "# -------------------- Función para Presentar los Resultados -------------------- #\n",
        "\n",
        "def display_results(results: Dict[str, List[ModelResult]]):\n",
        "    for model_name, model_results in results.items():\n",
        "        print(f\"\\n\\n===== Resultados para el modelo: {model_name} =====\\n\")\n",
        "        for res in model_results:\n",
        "            print(f\"Pregunta: {res.question}\")\n",
        "            print(f\"Respuesta Esperada: {res.expected_answer}\")\n",
        "            print(f\"Respuestas Correctas: {res.correct_responses} de {len(res.answers)}\")\n",
        "            print(f\"Consistencia: {res.consistency * 100:.2f}%\")\n",
        "            print(\"Detalles de las respuestas:\")\n",
        "            for ans in res.answers:\n",
        "                status = \"✅ Correcta\" if ans['is_correct'] else \"❌ Incorrecta\"\n",
        "                print(f\"  - Consulta {ans['run']}: {ans['answer']} | {status}\")\n",
        "            print(\"\\n-----------------------------------\\n\")\n",
        "\n",
        "# -------------------- Ejecución Principal -------------------- #\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejecutar el benchmark\n",
        "    benchmark_results = run_benchmark(models_to_evaluate, benchmark_questions, num_runs=5)\n",
        "\n",
        "    # Presentar los resultados\n",
        "    display_results(benchmark_results)\n",
        "\n",
        "    # Opcional: Guardar los resultados en un archivo JSON\n",
        "    with open(\"benchmark_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(benchmark_results, f, ensure_ascii=False, indent=4)\n",
        "    print(\"\\n✅ Resultados guardados en 'benchmark_results.json'\")\n"
      ]
    }
  ]
}