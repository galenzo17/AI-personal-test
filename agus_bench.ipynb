{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMd54xygakC5KvzuoBnKt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galenzo17/AI-personal-test/blob/main/agus_bench.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VEhTnSW4hci"
      },
      "outputs": [],
      "source": [
        "# Instalar la biblioteca de OpenAI si no está instalada\n",
        "!pip install openai\n",
        "\n",
        "import openai\n",
        "import time\n",
        "import json\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# -------------------- Configuración del Modelo y Autenticación -------------------- #\n",
        "\n",
        "# Define tu clave API de OpenAI aquí. Es recomendable usar variables de entorno para mayor seguridad.\n",
        "# Puedes subirla a Colab de forma segura o utilizar un gestor de secretos.\n",
        "openai.api_key = 'TU_CLAVE_API_AQUI'  # Reemplaza con tu clave API\n",
        "\n",
        "# Define el modelo que deseas evaluar\n",
        "MODEL_NAME = 'gpt-3.5-turbo'  # Cambia según el modelo que prefieras\n",
        "\n",
        "# -------------------- Definición de Preguntas y Respuestas Esperadas -------------------- #\n",
        "\n",
        "# Lista de preguntas para el benchmark\n",
        "benchmark_questions = [\n",
        "    {\n",
        "        \"question\": \"¿Cuál es la capital de Francia?\",\n",
        "        \"expected_answer\": \"París\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Resuelve la siguiente operación matemática: 15 * 12.\",\n",
        "        \"expected_answer\": \"180\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"¿Quién escribió 'Cien Años de Soledad'?\",\n",
        "        \"expected_answer\": \"Gabriel García Márquez\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Número de veces que se preguntará a cada modelo por cada pregunta\n",
        "NUM_RUNS = 5\n",
        "\n",
        "# -------------------- Función para Consultar al Modelo -------------------- #\n",
        "\n",
        "def query_model(question, model=MODEL_NAME, temperature=0):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Eres un asistente útil.\"},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        answer = response.choices[0].message['content'].strip()\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"Error al consultar al modelo: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------- Función para Evaluar la Respuesta -------------------- #\n",
        "\n",
        "def evaluate_answer(model_answer, expected_answer):\n",
        "    \"\"\"\n",
        "    Evalúa la respuesta del modelo comparándola con la respuesta esperada.\n",
        "    Utiliza una métrica de similitud básica.\n",
        "    \"\"\"\n",
        "    # Normalizar respuestas\n",
        "    model_answer_norm = model_answer.lower().strip()\n",
        "    expected_answer_norm = expected_answer.lower().strip()\n",
        "\n",
        "    # Comparación directa\n",
        "    if model_answer_norm == expected_answer_norm:\n",
        "        return True\n",
        "\n",
        "    # Comparación basada en similitud\n",
        "    similarity = SequenceMatcher(None, model_answer_norm, expected_answer_norm).ratio()\n",
        "    return similarity > 0.8  # Umbral de similitud del 80%\n",
        "\n",
        "# -------------------- Ejecución del Benchmark -------------------- #\n",
        "\n",
        "def run_benchmark():\n",
        "    results = []\n",
        "    for q in benchmark_questions:\n",
        "        question = q['question']\n",
        "        expected = q['expected_answer']\n",
        "        correct_count = 0\n",
        "        answers = []\n",
        "\n",
        "        print(f\"\\nEvaluando la pregunta: {question}\")\n",
        "        print(f\"Respuesta esperada: {expected}\")\n",
        "\n",
        "        for i in range(NUM_RUNS):\n",
        "            print(f\"  Consulta {i+1}/{NUM_RUNS}...\")\n",
        "            answer = query_model(question)\n",
        "            if answer is None:\n",
        "                print(\"    No se pudo obtener una respuesta.\")\n",
        "                continue\n",
        "            is_correct = evaluate_answer(answer, expected)\n",
        "            answers.append({\n",
        "                \"run\": i+1,\n",
        "                \"answer\": answer,\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "            if is_correct:\n",
        "                correct_count += 1\n",
        "            print(f\"    Respuesta: {answer} | Correcta: {is_correct}\")\n",
        "            time.sleep(1)  # Espera para evitar exceder los límites de la API\n",
        "\n",
        "        consistency = correct_count / NUM_RUNS\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"expected_answer\": expected,\n",
        "            \"correct_responses\": correct_count,\n",
        "            \"consistency\": consistency,\n",
        "            \"answers\": answers\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------- Presentación de Resultados -------------------- #\n",
        "\n",
        "def display_results(results):\n",
        "    print(\"\\n\\n===== Resultados del Benchmark =====\\n\")\n",
        "    for res in results:\n",
        "        print(f\"Pregunta: {res['question']}\")\n",
        "        print(f\"Respuesta Esperada: {res['expected_answer']}\")\n",
        "        print(f\"Respuestas Correctas: {res['correct_responses']} de {NUM_RUNS}\")\n",
        "        print(f\"Consistencia: {res['consistency']*100}%\")\n",
        "        print(\"Detalles de las respuestas:\")\n",
        "        for ans in res['answers']:\n",
        "            status = \"✅ Correcta\" if ans['is_correct'] else \"❌ Incorrecta\"\n",
        "            print(f\"  - Consulta {ans['run']}: {ans['answer']} | {status}\")\n",
        "        print(\"\\n-----------------------------------\\n\")\n",
        "\n",
        "# -------------------- Ejecución Principal -------------------- #\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_results = run_benchmark()\n",
        "    display_results(benchmark_results)\n"
      ]
    }
  ]
}