{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwmUFZCD+nbaxgw+qJmYwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galenzo17/AI-personal-test/blob/main/inventory_forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUnhKxQw933u"
      },
      "outputs": [],
      "source": [
        "# Instalar dependencias en Colab\n",
        "!pip install torch==2.0.1 transformers==4.31.0 sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Verificación de GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Usando dispositivo:\", device)\n",
        "\n",
        "# Cargar un modelo Transformer (GPT-2)\n",
        "model_name = \"gpt2\"  # Puedes usar variantes más grandes como gpt2-medium, etc.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "model.eval()\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# Ejemplo de datos sintéticos de inventario:\n",
        "# Supongamos que tienes datos diarios de inventario (unidades en stock),\n",
        "# por ejemplo, representadas como una serie de números.\n",
        "# Por simplicidad, convertiremos los números a texto y el modelo intentará\n",
        "# continuar la secuencia.\n",
        "#\n",
        "# En la vida real, deberías:\n",
        "# 1) Convertir tus datos de inventario en una secuencia textual estructurada.\n",
        "# 2) Probar distintos formateos (prompt engineering) que ayuden al modelo\n",
        "#    a entender la secuencia temporal.\n",
        "# 3) Ajustar un modelo con fine-tuning si es necesario.\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Generamos datos sintéticos de inventario (por ejemplo, 30 días de datos):\n",
        "import random\n",
        "random.seed(42)\n",
        "historical_inventories = [1000]  # Inventario inicial\n",
        "for i in range(1, 30):\n",
        "    # Simulamos variaciones diarias aleatorias\n",
        "    next_val = max(0, historical_inventories[-1] + random.randint(-50, 50))\n",
        "    historical_inventories.append(next_val)\n",
        "\n",
        "# Creamos un prompt que el modelo recibirá:\n",
        "# Ej: \"Inventario diario (en unidades): Día 1: 1000, Día 2: 980, Día 3: 1020, ...\"\n",
        "# Luego le pedimos que prediga el siguiente día.\n",
        "prompt_text = \"Inventario diario (en unidades): \" + \", \".join([f\"Día {i+1}: {inv}\" for i,inv in enumerate(historical_inventories)]) + \", Día 31:\"\n",
        "\n",
        "# Tokenizamos el prompt\n",
        "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
        "\n",
        "# Generamos el texto completado por el modelo\n",
        "# Aquí el modelo va a \"continuar\" la secuencia.\n",
        "# Ajusta max_length según la cantidad de pasos que quieras predecir.\n",
        "max_length = input_ids.shape[1] + 10  # Intentemos predecir unos tokens más adelante\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=0.7,      # Ajustable, controla la variabilidad\n",
        "        top_p=0.9,            # Ajustable, controla el filtrado por rango acumulado\n",
        "        do_sample=True,       # Si hacemos sampling para mayor diversidad\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Decodificamos el resultado\n",
        "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Prompt original:\\n\", prompt_text)\n",
        "print(\"\\nTexto generado:\\n\", generated_text)\n",
        "\n",
        "# Nota: El modelo GPT-2 no sabe que son datos de inventario,\n",
        "# simplemente continúa la secuencia textual. Podrás ver que\n",
        "# \"inventa\" valores o texto. Para un forecasting real,\n",
        "# necesitas:\n",
        "# - Entrenar con datos similares\n",
        "# - Ajustar el prompt\n",
        "# - Usar un modelo especializado en series temporales\n",
        "# - O Implementar un Transformer entrenado con librerías\n",
        "#   especializadas (ej. Darts, PyTorch Forecasting, etc.)\n"
      ]
    }
  ]
}